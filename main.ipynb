{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bbd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"文本预处理：清洗、分句、去除无关字符\"\"\"\n",
    "    # 分句\n",
    "    sentences = re.split(r'(?<=[。！？\\?])', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 5]\n",
    "    \n",
    "    # 清洗\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # 去除引号、括号等字符\n",
    "        cleaned = re.sub(r'[《》“”\\n（）()]', '', sentence)\n",
    "        cleaned_sentences.append(cleaned)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87935622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['据外媒报道，美国两架海军军机26日分别坠毁在南海，无人员伤亡。',\n",
       " '第一起坠机事件涉及一架MH-60R海鹰直升机。',\n",
       " '根据美国海军太平洋舰队的声明，这架直升机在尼米兹号航空母舰进行例行操作时坠入南海。',\n",
       " '声明称，直升机上的三名机组人员被搜救队救起。',\n",
       " '半小时后，一架波音F/A-18F超级大黄蜂战斗机在尼米兹号航空母舰执行例行任务时也坠毁在南海。',\n",
       " '机上两名机组人员成功弹射逃生，被安全救起。',\n",
       " '据美国海军称，所有相关人员均安全且情况稳定。',\n",
       " '两起事故原因正在调查中。',\n",
       " ' 军事专家张军社27日接受环球时报采访时表示，美国在南海一天内先后坠毁一架舰载战斗机和一架直升机，这一事件并非偶然。',\n",
       " '美军长期在南海、亚太及全球范围内维持高强度战备状态，不断进行军事部署，以维持其霸权地位和国际警察角色。',\n",
       " '长期高压运作使美军兵力紧张、人员疲惫，事故发生的风险自然随之上升。',\n",
       " '他认为，此次事故很可能与操作疏忽或过度疲劳等因素有关。',\n",
       " '军事专家宋忠平27日接受环球时报采访时也持相似观点。',\n",
       " '他指出，美军长期以所谓航行自由为借口，在南海频繁炫耀武力，意在彰显其军事存在。',\n",
       " '表面上看，美国作为军事霸主仍在维持强势姿态，但实际上，即便拥有11 艘航空母舰，面对如此繁重的任务，美军也已力不从心。',\n",
       " '宋忠平分析称，美军航母肩负全球部署和多重任务，长期在中东及其他地区高强度执行作战和训练，加之部分官兵存在懈怠、厌战情绪，导致安全风险上升。',\n",
       " '因此，同一天发生两起坠机事故虽令人震惊，但并不令人意外。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = read_text(\"text1.txt\")\n",
    "preprocess_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50746015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class DependencyTripleExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"zh_core_web_sm\")\n",
    "        # 加载领域词典\n",
    "        # self.load_domain_dictionary()\n",
    "        \n",
    "        # 初始化关系映射规则\n",
    "        self.init_relation_relus()\n",
    "    \n",
    "    def init_relation_relus(self):\n",
    "        \"\"\"初始化依存关系到语义关系的映射规则\"\"\"\n",
    "        # 基础依存关系映射\n",
    "        self.dep_to_relation = {\n",
    "            'nsubj': '',  # 主语，使用动词本身\n",
    "            'dobj': '',   # 宾语，使用动词本身\n",
    "            'nmod:poss': '的',  # 属格关系\n",
    "            'prep': '',   # 介词，与动词组合\n",
    "            'pobj': '',   # 介词宾语\n",
    "            'appos': '是',  # 同位语关系\n",
    "            'amod': '',   # 形容词修饰\n",
    "            'nummod': ''  # 数量修饰\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"文本预处理：清洗、分句、去除无关字符\"\"\"\n",
    "        # 分句\n",
    "        sentences = re.split(r'(?<=[。！？\\?])', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 5]\n",
    "        \n",
    "        # 清洗\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            # 去除引号、括号等字符\n",
    "            cleaned = re.sub(r'[《》“”\\n（）()]', '', sentence)\n",
    "            cleaned_sentences.append(cleaned)\n",
    "        return cleaned_sentences\n",
    "    \n",
    "    def analyze_sentence(self, sentence):\n",
    "        \"\"\"分析句子结构：分词、词性标注、依存句法分析\"\"\"\n",
    "        # 实验spacy进行依存分析\n",
    "        doc = self.nlp(sentence)\n",
    "        \n",
    "        # 构建依存关系图\n",
    "        dependency_info = []\n",
    "        root_token = None\n",
    "        for token in doc:\n",
    "            dep_info = {\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'dep': token.dep_,\n",
    "                'head_text': token.head.text,\n",
    "                'head_pos': token.head.pos_,\n",
    "                'is_root': token.dep == 'ROOT'\n",
    "            }\n",
    "            dependency_info.append(dep_info)\n",
    "            if token.dep == 'ROOT':\n",
    "                root_token = token\n",
    "        return doc, dependency_info, root_token\n",
    "    \n",
    "    def extract_triples_by_rules(self, doc, dependency_info, root_token):\n",
    "        \"\"\"基于规则的三元组抽取\"\"\"\n",
    "        triples = []\n",
    "\n",
    "        # 规则1：主谓宾结构\n",
    "        svo_triples = self.extract_svo_triples(doc, root_token)\n",
    "        triples.extend(svo_triples)\n",
    "\n",
    "        # 规则2：介词结构\n",
    "        prep_triples = self.extract_preposition_triples(doc)\n",
    "        triples.extend(prep_triples)\n",
    "\n",
    "        # 规则3：属格关系\n",
    "        poss_triples = self.extract_possessive_triples(doc)\n",
    "        triples.extend(poss_triples)\n",
    "\n",
    "        # 规则4：同位语关系\n",
    "        appos_triples = self.extract_appos_triples(doc)\n",
    "        triples.extend(appos_triples)\n",
    "\n",
    "        return triples\n",
    "    \n",
    "    def extract_svo_triples(self, doc, root_token):\n",
    "        \"\"\"抽取主谓宾结构的三元组\"\"\"\n",
    "        triples = []\n",
    "        # 找到所有动词\n",
    "        verbs = [token for token in doc if token.pos_ == 'VERB']\n",
    "        if not verbs and root_token:\n",
    "            verbs = [root_token]\n",
    "        \n",
    "        for verb in verbs:\n",
    "            # 寻找主语\n",
    "            subjects = []\n",
    "            for child in verb.children:\n",
    "                if child.dep_ in ['nsubj', 'nsubj:pass']:\n",
    "                    subjects.append(child)\n",
    "            \n",
    "            # 寻找宾语\n",
    "            objects = []\n",
    "            for child in verb.children:\n",
    "                if child.dep_ in ['dobj', 'obj']:\n",
    "                    objects.append(child)\n",
    "                elif child.dep_ == 'prep':\n",
    "                    objects.append(child)\n",
    "            \n",
    "            # 生成三元组\n",
    "            for subject in subjects:\n",
    "                for object in objects:\n",
    "                    # 获取完整的短语\n",
    "                    subject_phrase = self.get_complete_phrase(subject)\n",
    "                    object_phrase = self.get_complete_phrase(object)\n",
    "                    relation = verb.text\n",
    "\n",
    "                    # 验证是否合理\n",
    "                    if self.is_valid_triple(subject_phrase, relation, object_phrase):\n",
    "                        triple = {\n",
    "                            'subject': subject_phrase,\n",
    "                            'relation': relation,\n",
    "                            'object': object_phrase,\n",
    "                            'subject_type': self.classify_entity_type(subject),\n",
    "                            'object_type': self.classify_entity_type(object),\n",
    "                            'confidence': 0.8,\n",
    "                            'rule': 'SVO'\n",
    "                        }\n",
    "                        triples.append(triple)\n",
    "        return triples\n",
    "    \n",
    "    def extract_preposition_triples(self, doc):\n",
    "        \"\"\"抽取介词结构的三元组\"\"\"\n",
    "        triples = []\n",
    "        # 方法1：基于介词修饰关系\n",
    "        triples.extend(self._extract_prep_triples(doc))\n",
    "        # 方法2：基于名词修饰关系（nmod）\n",
    "        triples.extend(self._extract_nmod_triples(doc))\n",
    "        # 方法3：基于被动语态的施事关系\n",
    "        # triples.extend(self._extract_agent_triples(doc))\n",
    "\n",
    "        return triples\n",
    "\n",
    "    def _extract_prep_triples(self, doc):\n",
    "        \"\"\"抽取基于prep关系的介词结构\"\"\"\n",
    "        print(\"Extracting preposition triples...\")\n",
    "        triples = []\n",
    "        # 找到所有介词\n",
    "        prepositions = [token for token in doc if token.dep_ == 'prep']\n",
    "        for prep in prepositions:\n",
    "            head_verb = prep.head\n",
    "            print(f\"head_verb: {head_verb.text}\")\n",
    "            # 介词的宾语\n",
    "            pobjects = [child for child in prep.children if child.dep_ == 'pobj']\n",
    "            print(f\"pobjects: {[p.text for p in pobjects]}\")\n",
    "            if pobjects and head_verb.pos_ == 'VERB':\n",
    "                # 找到动词的主语\n",
    "                subjects = [child for child in head_verb.children if child.dep_ in ['nsubj', 'nsubj:pass']]\n",
    "                for subject in subjects:\n",
    "                    for pobj in pobjects:\n",
    "                        subject_phrase = self.get_complete_phrase(subject)\n",
    "                        object_phrase = self.get_complete_phrase(pobj)\n",
    "                        # 关系 = 动词 + 介词\n",
    "                        relation = f\"{head_verb.text}{prep.text}\"\n",
    "                        print(f\"relation: {relation}\")\n",
    "                        # 验证是否合理\n",
    "                        if self.is_valid_triple(subject_phrase, relation, object_phrase):\n",
    "                            triple = {\n",
    "                                'subject': subject_phrase,\n",
    "                                'relation': relation,\n",
    "                                'object': object_phrase,\n",
    "                                'subject_type': self.classify_entity_type(subject),\n",
    "                                'object_type': self.classify_entity_type(object),\n",
    "                                'confidence': 0.7,\n",
    "                                'rule': 'PREP'\n",
    "                            }\n",
    "                        triples.append(triple)\n",
    "\n",
    "    def _extract_nmod_triples(self, doc):\n",
    "        \"\"\"抽取基于nmod关系的介词结构\"\"\"\n",
    "        triples = []\n",
    "        for token in doc:\n",
    "            if token.dep_ in ['nmod', 'nmod:prep']:\n",
    "                print(f\"nmod token: {token.text}\")\n",
    "                head_noun = token.head\n",
    "                modifier = token\n",
    "\n",
    "                # 检查是否有介词标记\n",
    "                preposition = None\n",
    "                for child in modifier.children:\n",
    "                    if child.dep_ == 'case':\n",
    "                        preposition = child.text\n",
    "                        break\n",
    "                \n",
    "                if preposition:\n",
    "                    subject_phrase = self.get_complete_phrase(head_noun)\n",
    "                    object_phrase = self.get_complete_phrase(modifier)\n",
    "                    if self.is_valid_triple(subject_phrase, preposition, object_phrase):\n",
    "                        triple = {\n",
    "                            'subject': subject_phrase,\n",
    "                            'relation': preposition,\n",
    "                            'object': object_phrase,\n",
    "                            'subject_type': self.classify_entity_type(head_noun),\n",
    "                            'object_type': self.classify_entity_type(modifier),\n",
    "                            'confidence': 0.6,\n",
    "                            'rule': 'PREP'\n",
    "                        }\n",
    "                        triples.append(triple)\n",
    "        return triples\n",
    "\n",
    "    def extract_possessive_triples(self, doc):\n",
    "        \"\"\"抽取属格关系的三元组\"\"\"\n",
    "        triples = []\n",
    "        # 找到所有属格关系\n",
    "        poss_relations = [token for token in doc if token.dep_ in ['poss', 'nmod:poss']]\n",
    "        \n",
    "\n",
    "    def get_complete_phrase(self, token):\n",
    "        \"\"\"获取完整的短语\"\"\"\n",
    "        phrase_tokens = []\n",
    "        \n",
    "        # 使用栈进行深度优先遍历\n",
    "        stack = [token]\n",
    "        visited = set()\n",
    "\n",
    "        while stack:\n",
    "            current_token = stack.pop()\n",
    "            if current_token.i in visited:\n",
    "                continue\n",
    "            visited.add(current_token.i)\n",
    "            # 添加当前token\n",
    "            phrase_tokens.append((current_token.i, current_token.text))\n",
    "            # 添加所有修饰当前token的子节点\n",
    "            for child in current_token.children:\n",
    "                if child.dep_ in ['amod', 'nummod', 'compound', 'nmod']:\n",
    "                    stack.append(child)\n",
    "        \n",
    "        # 按原始顺序排序并组合\n",
    "        phrase_tokens.sort(key=lambda x: x[0])\n",
    "        phrase = ''.join([text for _, text in phrase_tokens])\n",
    "\n",
    "        return phrase\n",
    "\n",
    "    def classify_entity_type(self, token):\n",
    "        \"\"\"基于词性和依存关系分类实体类型\"\"\"\n",
    "\n",
    "        if token.pos_ in ['PROPN']: # 专有名词\n",
    "            return 'ENTITY'\n",
    "        elif token.pos_ == 'NOUN': # 普通名词\n",
    "            return 'NOUN'\n",
    "        elif token.pos_ == 'VERB': # 动词\n",
    "            return 'ACTION'\n",
    "        elif token.ent_type_ != '': # 已识别的实体类型\n",
    "            return token.ent_type_\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    def is_valid_triple(self, subject, relation, object):\n",
    "        \"\"\"验证三元组的有效性\"\"\"\n",
    "        # 检查长度\n",
    "        if len(subject) < 2 or len(object) < 2:\n",
    "            return False\n",
    "        # 检查关系有效性\n",
    "        if len(relation) == 0 or relation in ['', ' ']:\n",
    "            return False\n",
    "        # 检查是否为无意义的组合\n",
    "        meaningless = ['是是', '的的', '在在']\n",
    "        for m in meaningless:\n",
    "            if m in (subject + relation + object):\n",
    "                return False\n",
    "        # 代词/指示词过滤（单独作为实体不够具体）\n",
    "        # pronouns = {'他','她','它','他们','我们','你','我','这','该','其','它们','这里','那里','这些','那些','这个','那个'}\n",
    "        # if subject in pronouns or object in pronouns:\n",
    "        #     return False\n",
    "        # 检查主体和客体是否相同\n",
    "        if subject == object:\n",
    "            return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56a4f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmod token: 报道\n",
      "nmod token: 美国\n",
      "nmod token: 南海\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'subject': '坠毁',\n",
       "  'relation': '据',\n",
       "  'object': '报道',\n",
       "  'subject_type': 'ACTION',\n",
       "  'object_type': 'ACTION',\n",
       "  'confidence': 0.6,\n",
       "  'rule': 'PREP'},\n",
       " {'subject': '坠毁',\n",
       "  'relation': '在',\n",
       "  'object': '南海',\n",
       "  'subject_type': 'ACTION',\n",
       "  'object_type': 'ENTITY',\n",
       "  'confidence': 0.6,\n",
       "  'rule': 'PREP'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DependencyTripleExtractor()\n",
    "sentences = model.preprocess_text(text1)\n",
    "doc, dependency_info, root_token = model.analyze_sentence(sentences[0])\n",
    "triples = model._extract_nmod_triples(doc)\n",
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf2324b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc, dependency_info, root_token = model.analyze_sentence(\"学生们在图书馆认真学习\")\n",
    "doc, dependency_info, root_token = model.analyze_sentence(doc)\n",
    "triples = model.extract_svo_triples(doc, root_token)\n",
    "print(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "242a9c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词: 据, 依存关系: case, 父节点: 报道\n",
      "词: 外媒, 依存关系: nsubj, 父节点: 报道\n",
      "词: 报道, 依存关系: nmod:prep, 父节点: 坠毁\n",
      "词: ，, 依存关系: punct, 父节点: 坠毁\n",
      "词: 美国, 依存关系: nmod, 父节点: 军机\n",
      "词: 两, 依存关系: nummod, 父节点: 军机\n",
      "词: 架, 依存关系: mark:clf, 父节点: 两\n",
      "词: 海军, 依存关系: compound:nn, 父节点: 军机\n",
      "词: 军机, 依存关系: nsubj, 父节点: 坠毁\n",
      "词: 26日, 依存关系: nmod:tmod, 父节点: 坠毁\n",
      "词: 分别, 依存关系: advmod, 父节点: 坠毁\n",
      "词: 坠毁, 依存关系: ROOT, 父节点: 坠毁\n",
      "词: 在, 依存关系: case, 父节点: 南海\n",
      "词: 南海, 依存关系: nmod:prep, 父节点: 坠毁\n",
      "词: ，, 依存关系: punct, 父节点: 坠毁\n",
      "词: 无, 依存关系: conj, 父节点: 坠毁\n",
      "词: 人员, 依存关系: nsubj, 父节点: 伤亡\n",
      "词: 伤亡, 依存关系: ccomp, 父节点: 无\n",
      "词: 。, 依存关系: punct, 父节点: 坠毁\n"
     ]
    }
   ],
   "source": [
    "# 查看依存关系\n",
    "doc = model.nlp(sentences[0])\n",
    "for token in doc:\n",
    "    print(f\"词: {token.text}, 依存关系: {token.dep_}, 父节点: {token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53dfc1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'据外媒报道，美国两架海军军机26日分别坠毁在南海，无人员伤亡。'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cec49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
